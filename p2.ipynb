{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNX/1nTujrcBaxNtf1zuqZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuntahaZaigham1/ds1/blob/master/p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDLZsMrYcvH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdCMiL3Fa-td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hypothesis(theta, X, n):\n",
        "    h = np.ones((X.shape[0],1))\n",
        "    theta = theta.reshape(1,n+1)\n",
        "    for i in range(0,X.shape[0]):\n",
        "        h[i]=float(np.matmul(theta, X[i]))\n",
        "    h = h.reshape(X.shape[0])\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcHAlsGzb1m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BGD(theta, alpha, num_iters, h, X, y, n):\n",
        "    cost = np.ones(num_iters)\n",
        "    for i in range(0,num_iters):\n",
        "        theta[0] = theta[0]-((alpha/X.shape[0])*sum(np.square(h - y)))\n",
        "        for j in range(1,n+1):\n",
        "            theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])\n",
        "        h = hypothesis(theta, X, n)\n",
        "        cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))\n",
        "    theta = theta.reshape(1,n+1)\n",
        "    return theta, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfqaMi7fcO_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_regression(X, y, alpha, num_iters):\n",
        "    n = X.shape[1]\n",
        "    one_column = np.ones((X.shape[0],1))\n",
        "    X = np.concatenate((one_column, X), axis = 1)\n",
        "    # initializing the parameter vector...\n",
        "    theta = np.zeros(n+1,dtype=object)\n",
        "    print(theta)\n",
        "    # hypothesis calculation....\n",
        "    h = hypothesis(theta, X, n)\n",
        "    # returning the optimized parameters by Gradient Descent...\n",
        "    theta, cost = BGD(theta,alpha,num_iters,h,X,y,n)\n",
        "    return theta, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVeDmOb8cZWO",
        "colab_type": "code",
        "outputId": "97371dda-16fb-44e1-9369-4e003b0f445a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data = np.loadtxt('/content/multivariateLRData.csv', delimiter=',')\n",
        "#data = pd.read_csv(\"/content/multivariateLRData.csv\") \n",
        "data = np.hstack(( np.ones((len(data), 1)), data))# adding x0\n",
        "\n",
        "No_of_cols = data.shape[1]\n",
        "X_train = data[:,range(0, No_of_cols - 1)] #feature set\n",
        "y_train = data[:, [No_of_cols - 1]] #label set\n",
        "\n",
        "print(X_train)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1.  24. 100. 110.  73.]\n",
            " [  1.  19. 100.  78.  30.]\n",
            " [  1.  13.  88.  99.  99.]\n",
            " [  1.  21. 100.  68.  30.]\n",
            " [  1.  17.  90.  69.  36.]\n",
            " [  1.  10.  88.  22.  22.]\n",
            " [  1.  19.  94.  53.  17.]\n",
            " [  1.  14.  90.  37.  25.]\n",
            " [  1.  18. 100.  38.  13.]\n",
            " [  1.  18. 100.  38.  13.]\n",
            " [  1.   9.   0.   7.   7.]\n",
            " [  1.   6.  59.  33.  25.]\n",
            " [  1.   5.   0.   7.   7.]\n",
            " [  1.   4.  75.   8.   8.]\n",
            " [  1.   4.   0.   6.   6.]\n",
            " [  1.  24.  96. 140.  35.]\n",
            " [  1.  23.  92.  53.  25.]\n",
            " [  1.   9.  28.  31.  23.]\n",
            " [  1.   2.  94.  52.  17.]\n",
            " [  1.   6.  80.  15.  15.]\n",
            " [  1.   5.  25.   9.   9.]\n",
            " [  1.   0. 100.   0.   0.]\n",
            " [  1.   0. 100.   0.   0.]\n",
            " [  1.   8.  58.  37.  34.]\n",
            " [  1.  15.  77.  12.  12.]\n",
            " [  1.  11.  76.  28.  28.]\n",
            " [  1.  12.  55.  24.  20.]\n",
            " [  1.   4. 100.  11.  11.]\n",
            " [  1.  14.  94. 198.  24.]\n",
            " [  1.   2.  85.  11.   7.]\n",
            " [  1.   1.   0.   3.   3.]\n",
            " [  1.   4.  90.  20.  20.]\n",
            " [  1.   3.  90.  11.   8.]\n",
            " [  1.   5.   0.  10.   5.]\n",
            " [  1.   5. 100.   8.   8.]\n",
            " [  1.   0.   0.   3.   3.]\n",
            " [  1.   3. 100.  38.  11.]\n",
            " [  1.   0.  50.   2.   2.]\n",
            " [  1.   1. 100.  30.  20.]\n",
            " [  1.  16.  81.  28.  20.]\n",
            " [  1.   3.  67.  17.  17.]\n",
            " [  1.   9.  97.  38.  11.]\n",
            " [  1.   3.  38.   7.   7.]\n",
            " [  1.  10.  82.  10.  10.]\n",
            " [  1.  11.   0.  41.   9.]\n",
            " [  1.   4.   0.   5.   5.]\n",
            " [  1.   4.   0.   5.   5.]\n",
            " [  1.   4.   0.   5.   5.]\n",
            " [  1.  14.  73.  92.  23.]\n",
            " [  1.  12.  89.  55.  21.]\n",
            " [  1.   0. 100.  98.  98.]\n",
            " [  1.  13.  75.  36.   8.]\n",
            " [  1.  10.  96.  98.  98.]\n",
            " [  1.  14.  96.  48.  18.]\n",
            " [  1.  10.  46.  23.  15.]\n",
            " [  1.   5. 100.  18.  18.]\n",
            " [  1.   5.  78.  14.  14.]\n",
            " [  1.   4.   0.   8.   8.]\n",
            " [  1.   1.   0.   2.   2.]\n",
            " [  1.  14.  87.  61.  26.]\n",
            " [  1.  14.  96.  54.   8.]\n",
            " [  1.  18.  83.  58.  10.]\n",
            " [  1.  14.  91.  54.   8.]\n",
            " [  1.  17.  93.  54.   6.]\n",
            " [  1.  17.  93.  54.   6.]\n",
            " [  1.  14.  81.  46.  11.]\n",
            " [  1.  12.  80.  37.  10.]\n",
            " [  1.   6.  95.  59.  24.]\n",
            " [  1.  15.  94.  44.   9.]\n",
            " [  1.  14.  86.  37.  10.]\n",
            " [  1.  10.  85.  37.   9.]\n",
            " [  1.   1. 100.  53.  22.]\n",
            " [  1.   1. 100.  20.  16.]\n",
            " [  1.   1. 100.  33.  14.]\n",
            " [  1.   0.   0.   9.   9.]\n",
            " [  1.  14.  97.  53.  22.]\n",
            " [  1.  10.  95.  42.  15.]\n",
            " [  1.   9.  86. 222. 100.]\n",
            " [  1.  20.  95.  85.  28.]\n",
            " [  1.  12.  83.  52.  39.]\n",
            " [  1.  13.  80.  45.  37.]\n",
            " [  1.  16.  87.  40.  12.]\n",
            " [  1.   9.   0.  18.  18.]\n",
            " [  1.   7.   0.  15.  15.]\n",
            " [  1.   9.  93.  49.  14.]\n",
            " [  1.   4.   0.  16.  16.]\n",
            " [  1.   3.   0.   8.   8.]\n",
            " [  1.   6.  62.   8.   8.]\n",
            " [  1.   1.  75.   6.   6.]\n",
            " [  1.  22.  94.  31.  31.]\n",
            " [  1.  16.  93.  38.  32.]\n",
            " [  1.  19.  90.  35.  35.]\n",
            " [  1.   6.  74.  48.  21.]\n",
            " [  1.   9.  83.  14.  14.]\n",
            " [  1.   4.  84.  33.  13.]\n",
            " [  1.  16.   0.  10.  10.]\n",
            " [  1.   8.  68.   7.   7.]\n",
            " [  1.   9.   0.  10.  10.]\n",
            " [  1.   7.  75.   6.   6.]\n",
            " [  1.   5.   0.  60.   5.]\n",
            " [  1.   5.   0.  24.   5.]\n",
            " [  1.   1. 100.  12.   5.]\n",
            " [  1.  15.  90.  34.  19.]\n",
            " [  1.  14.  72.  27.  12.]\n",
            " [  1.  14.  84.  33.  20.]\n",
            " [  1.  12.  67.  33.  14.]\n",
            " [  1.  11.  80.  28.  15.]\n",
            " [  1.  10.  88.  23.  23.]\n",
            " [  1.  13.  86.  27.  11.]\n",
            " [  1.  13. 100.  25.  19.]\n",
            " [  1.   0. 100.   4.   3.]\n",
            " [  1.   5.  86.  24.  24.]\n",
            " [  1.  10.  75.  20.  16.]\n",
            " [  1.  10.  88.  25.  25.]\n",
            " [  1.  12.  77.  13.   9.]\n",
            " [  1.   4.   0.   6.   6.]\n",
            " [  1.   2.   0.   9.   9.]\n",
            " [  1.   0. 100.   0.   0.]\n",
            " [  1.   0. 100.   0.   0.]\n",
            " [  1.  10.  96.  84.  50.]\n",
            " [  1.  15.  78.  11.  11.]\n",
            " [  1.  14.  73.   8.   8.]\n",
            " [  1.   1. 100.  38.  32.]\n",
            " [  1.  14.   0.  41.  10.]\n",
            " [  1.   0. 100.  35.  35.]\n",
            " [  1.  15.  78. 119.   7.]\n",
            " [  1.  15.  82.   7.   7.]\n",
            " [  1.   9.  83.  15.  15.]\n",
            " [  1.   3. 100.  32.  32.]\n",
            " [  1.   8.  72.  36.  11.]\n",
            " [  1.   8.  22.  11.  11.]\n",
            " [  1.   4.  50.  11.   6.]\n",
            " [  1.   0. 100.   7.   7.]\n",
            " [  1.   3. 100.  38.  11.]\n",
            " [  1.   0. 100.  10.  10.]\n",
            " [  1.   1.  70.   5.   5.]\n",
            " [  1.   1. 100.  30.  20.]\n",
            " [  1.   0.   0.  10.  10.]\n",
            " [  1.   0.   0.  10.  10.]\n",
            " [  1.   0. 100.  28.  28.]\n",
            " [  1.   1.   0.   2.   2.]\n",
            " [  1.   1. 100. 198.  24.]\n",
            " [  1.   0.   0.  60.   5.]\n",
            " [  1.   0.  98.  37.  34.]\n",
            " [  1.   0. 100.   7.   7.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phvU9O-yl_aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calling the principal function with learning_rate = 0.0001 and \n",
        "# num_iters = 300000\n",
        "theta, cost = linear_regression(X_train, y_train,0.01, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCDvgk9zPLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbl3YjRayi4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "cost = list(cost)\n",
        "n_iterations = [x for x in range(1,300001)]\n",
        "plt.plot(n_iterations, cost)\n",
        "plt.xlabel('No. of iterations')\n",
        "plt.ylabel('Cost')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}